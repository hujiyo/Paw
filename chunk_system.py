#!/usr/bin/env python
"""
è¯­å—ç³»ç»Ÿ - æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†
æ¯ä¸ªè¯­å—éƒ½çŸ¥é“è‡ªå·±çš„æ¥æº
"""

from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Dict, Any, Optional, Literal
from enum import Enum
from colorama import Fore, Style


class ChunkType(Enum):
    """è¯­å—ç±»å‹æšä¸¾"""
    SYSTEM = "system"          # ç³»ç»Ÿæç¤ºè¯
    MEMORY = "memory"          # æ³¨å…¥çš„è®°å¿†
    USER = "user"              # ç”¨æˆ·è¾“å…¥
    ASSISTANT = "assistant"    # AIç”Ÿæˆ
    THOUGHT = "thought"        # AIå†…éƒ¨æ€è€ƒ
    TOOL_CALL = "tool_call"    # å·¥å…·è°ƒç”¨
    TOOL_RESULT = "tool_result"  # å·¥å…·ç»“æœ


@dataclass
class Chunk:
    """
    è¯­å— - ä¸Šä¸‹æ–‡çš„åŸºæœ¬å•å…ƒ
    
    æ¯ä¸ªè¯­å—åœ¨åˆ›å»ºæ—¶å°±çŸ¥é“è‡ªå·±çš„ç±»å‹ï¼Œ
    ä¸éœ€è¦é€šè¿‡æ£€æµ‹å…³é”®è¯æ¥åˆ¤æ–­
    """
    content: str                    # å†…å®¹
    chunk_type: ChunkType           # ç±»å‹
    timestamp: datetime = field(default_factory=datetime.now)
    tokens: int = 0                 # tokenæ•°é‡
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __str__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤ºï¼ˆä¸å¸¦é¢œè‰²ï¼‰"""
        return self.content
    
    def colored_str(self) -> str:
        """å¸¦é¢œè‰²çš„å­—ç¬¦ä¸²è¡¨ç¤º"""
        color_map = {
            ChunkType.SYSTEM: Fore.RED,       # ç³»ç»Ÿæ³¨å…¥ - çº¢è‰²
            ChunkType.MEMORY: Fore.RED,       # è®°å¿†æ³¨å…¥ - çº¢è‰²
            ChunkType.USER: Fore.YELLOW,      # ç”¨æˆ·è¾“å…¥ - é»„è‰²
            ChunkType.ASSISTANT: Fore.GREEN,  # AIç”Ÿæˆ - ç»¿è‰²
            ChunkType.THOUGHT: Fore.CYAN,     # å†…éƒ¨æ€è€ƒ - é’è‰²
            ChunkType.TOOL_CALL: Fore.MAGENTA,    # å·¥å…·è°ƒç”¨ - ç´«è‰²
            ChunkType.TOOL_RESULT: Fore.BLUE,     # å·¥å…·ç»“æœ - è“è‰²
        }
        
        color = color_map.get(self.chunk_type, Fore.WHITE)
        return f"{color}{self.content}{Style.RESET_ALL}"
    
    def estimate_tokens(self) -> int:
        """ä¼°ç®—tokenæ•°é‡"""
        if self.tokens > 0:
            return self.tokens
        # ç²—ç•¥ä¼°ç®—ï¼š4ä¸ªå­—ç¬¦çº¦1ä¸ªtoken
        self.tokens = len(self.content) // 4
        return self.tokens


class ChunkManager:
    """
    è¯­å—ç®¡ç†å™¨ - ç®¡ç†æ•´ä¸ªå¯¹è¯çš„ä¸Šä¸‹æ–‡
    """
    
    def __init__(self, max_tokens: int = 64000, tools_schema: Optional[List[Dict]] = None):
        """åˆå§‹åŒ–
        
        Args:
            max_tokens: æœ€å¤§tokenæ•°
            tools_schema: å·¥å…·å®šä¹‰schemaï¼ˆOpenAIæ ¼å¼ï¼‰
        """
        self.chunks: List[Chunk] = []
        self.max_tokens = max_tokens
        self.current_tokens = 0
        self.tools_schema = tools_schema or []
        self.tools_tokens = self._estimate_tools_tokens()
    
    def _estimate_tools_tokens(self) -> int:
        """ä¼°ç®—å·¥å…·schemaçš„tokenæ•°"""
        if not self.tools_schema:
            return 0
        import json
        tools_json = json.dumps(self.tools_schema, ensure_ascii=False)
        # ç²—ç•¥ä¼°ç®—ï¼š4ä¸ªå­—ç¬¦çº¦1ä¸ªtoken
        return len(tools_json) // 4
    
    def add_chunk(self, content: str, chunk_type: ChunkType, 
                  metadata: Optional[Dict[str, Any]] = None) -> Chunk:
        """
        æ·»åŠ è¯­å—
        
        åœ¨æ·»åŠ æ—¶å°±æ˜ç¡®æ ‡è®°ç±»å‹ï¼Œä¸éœ€è¦åç»­çŒœæµ‹
        """
        chunk = Chunk(
            content=content,
            chunk_type=chunk_type,
            metadata=metadata or {}
        )
        chunk.estimate_tokens()
        self.chunks.append(chunk)
        self.current_tokens += chunk.tokens
        return chunk
    
    def add_system_prompt(self, prompt: str) -> Chunk:
        """æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼ˆæ³¨å…¥ï¼‰"""
        return self.add_chunk(prompt, ChunkType.SYSTEM)

    def update_latest_system_prompt(self, prompt: str) -> Chunk:
        """æ›´æ–°æœ€è¿‘çš„ç³»ç»Ÿæç¤ºè¯å†…å®¹"""
        for chunk in reversed(self.chunks):
            if chunk.chunk_type == ChunkType.SYSTEM:
                old_tokens = chunk.tokens
                chunk.content = prompt
                chunk.tokens = 0
                chunk.estimate_tokens()
                self.current_tokens += chunk.tokens - old_tokens
                return chunk
        return self.add_system_prompt(prompt)
    
    def add_memory(self, memory: str) -> Chunk:
        """æ·»åŠ è®°å¿†ï¼ˆæ³¨å…¥ï¼‰"""
        return self.add_chunk(memory, ChunkType.MEMORY)
    
    def add_user_input(self, input_text: str) -> Chunk:
        """æ·»åŠ ç”¨æˆ·è¾“å…¥"""
        return self.add_chunk(input_text, ChunkType.USER)
    
    def add_assistant_response(self, response: str) -> Chunk:
        """æ·»åŠ AIå›å¤ï¼ˆç”Ÿæˆï¼‰"""
        return self.add_chunk(response, ChunkType.ASSISTANT)
    
    def add_thought(self, thought: str) -> Chunk:
        """æ·»åŠ AIæ€è€ƒï¼ˆå†…éƒ¨ï¼‰"""
        return self.add_chunk(thought, ChunkType.THOUGHT)
    
    def add_tool_call(self, tool_info: str) -> Chunk:
        """æ·»åŠ å·¥å…·è°ƒç”¨"""
        return self.add_chunk(tool_info, ChunkType.TOOL_CALL)
    
    def add_tool_result(self, result: str) -> Chunk:
        """æ·»åŠ å·¥å…·ç»“æœ"""
        return self.add_chunk(result, ChunkType.TOOL_RESULT)
    
    def get_context_for_llm(self) -> List[Dict[str, str]]:
        """
        è·å–ç”¨äºLLMçš„ä¸Šä¸‹æ–‡
        
        å°†è¯­å—è½¬æ¢ä¸ºæ¶ˆæ¯æ ¼å¼ï¼Œä½†ä¿ç•™è¯­å—ä¿¡æ¯
        """
        messages = []
        
        # åˆå¹¶ç›¸åŒè§’è‰²çš„è¿ç»­è¯­å—
        current_role = None
        current_content = []
        
        for chunk in self.chunks:
            # è·³è¿‡æ€è€ƒè¯­å—å’Œå·¥å…·è°ƒç”¨è¯­å—ï¼ˆFunction Callingä¸éœ€è¦ï¼‰
            if chunk.chunk_type in [ChunkType.THOUGHT, ChunkType.TOOL_CALL]:
                continue
            
            # ç¡®å®šè§’è‰²
            if chunk.chunk_type in [ChunkType.SYSTEM, ChunkType.MEMORY]:
                role = "system"
            elif chunk.chunk_type == ChunkType.USER:
                role = "user"
            elif chunk.chunk_type == ChunkType.ASSISTANT:
                role = "assistant"
            elif chunk.chunk_type == ChunkType.TOOL_RESULT:
                # å·¥å…·ç»“æœä½¿ç”¨ tool è§’è‰²ï¼ˆOpenAI Function Calling æ ‡å‡†ï¼‰
                role = "tool"
            else:
                continue
            
            # å¦‚æœè§’è‰²å˜åŒ–ï¼Œä¿å­˜å½“å‰æ¶ˆæ¯
            if role != current_role and current_content:
                messages.append({
                    "role": current_role,
                    "content": "\n".join(current_content)
                })
                current_content = []
            
            current_role = role
            current_content.append(chunk.content)
        
        # ä¿å­˜æœ€åçš„æ¶ˆæ¯
        if current_content:
            messages.append({
                "role": current_role,
                "content": "\n".join(current_content)
            })
        
        return messages
    
    def print_context(self, show_types: bool = True, use_colors: bool = True, show_llm_view: bool = True):
        """
        æ‰“å°å®Œæ•´ä¸Šä¸‹æ–‡
        
        Args:
            show_types: æ˜¯å¦æ˜¾ç¤ºè¯­å—ç±»å‹
            use_colors: æ˜¯å¦ä½¿ç”¨é¢œè‰²
            show_llm_view: æ˜¯å¦æ˜¾ç¤ºLLMå®é™…çœ‹åˆ°çš„æ¶ˆæ¯ï¼ˆåˆå¹¶åï¼‰
        """
        print("\n" + "="*60)
        print("ğŸ“š å®Œæ•´ä¸Šä¸‹æ–‡")
        print("="*60)
        
        # æ˜¾ç¤ºå·¥å…·å®šä¹‰
        if self.tools_schema:
            tool_label = "[TOOLS]"
            if use_colors:
                tool_label = f"{Fore.MAGENTA}{tool_label}{Style.RESET_ALL}"
            print(f"\n{tool_label}")
            tool_names = [t['function']['name'] for t in self.tools_schema]
            tools_summary = f"å¯ç”¨å·¥å…· ({len(tool_names)}ä¸ª): {', '.join(tool_names)}"
            if use_colors:
                tools_summary = f"{Fore.MAGENTA}{tools_summary}{Style.RESET_ALL}"
            print(tools_summary)
        
        if show_llm_view:
            # æ˜¾ç¤ºåŸå§‹è¯­å—ï¼ˆä¿ç•™ç±»å‹ä¿¡æ¯ï¼‰
            for chunk in self.chunks:
                # è·³è¿‡æ€è€ƒå’Œå·¥å…·è°ƒç”¨ï¼ˆä¸å‘é€ç»™LLMï¼‰
                if chunk.chunk_type in [ChunkType.THOUGHT, ChunkType.TOOL_CALL]:
                    continue
                
                type_label = f"[{chunk.chunk_type.value.upper()}]"
                if use_colors:
                    if chunk.chunk_type in [ChunkType.SYSTEM, ChunkType.MEMORY]:
                        type_label = f"{Fore.YELLOW}{type_label}{Style.RESET_ALL}"
                    elif chunk.chunk_type == ChunkType.USER:
                        type_label = f"{Fore.CYAN}{type_label}{Style.RESET_ALL}"
                    elif chunk.chunk_type == ChunkType.ASSISTANT:
                        type_label = f"{Fore.GREEN}{type_label}{Style.RESET_ALL}"
                    elif chunk.chunk_type == ChunkType.TOOL_RESULT:
                        type_label = f"{Fore.BLUE}{type_label}{Style.RESET_ALL}"
                
                print(f"\n{type_label}")
                print(chunk.content)
        else:
            # æ˜¾ç¤ºåŸå§‹è¯­å—
            for i, chunk in enumerate(self.chunks):
                # ç±»å‹æ ‡ç­¾
                if show_types:
                    type_label = f"[{chunk.chunk_type.value.upper()}]"
                    if use_colors:
                        type_label = f"{Fore.CYAN}{type_label}{Style.RESET_ALL}"
                    print(f"\n{type_label}")
                
                # å†…å®¹
                if use_colors:
                    print(chunk.colored_str())
                else:
                    print(chunk.content)
                
                # å…ƒæ•°æ®
                if chunk.metadata:
                    meta_str = f"  ğŸ“ {chunk.metadata}"
                    if use_colors:
                        meta_str = f"{Fore.CYAN}{meta_str}{Style.RESET_ALL}"
                    print(meta_str)
        
        print("\n" + "="*60)
        total_tokens = self.current_tokens + self.tools_tokens
        print(f"æ¶ˆæ¯Tokenæ•°: {self.current_tokens}")
        if self.tools_tokens > 0:
            print(f"å·¥å…·Tokenæ•°: {self.tools_tokens}")
        print(f"æ€»Tokenæ•°: {total_tokens}/{self.max_tokens} "
              f"({total_tokens/self.max_tokens*100:.1f}%)")
        print("="*60)
    
    def print_mixed_response(self, response_chunks: List[Chunk]):
        """
        æ‰“å°æ··åˆå“åº”ï¼ˆä¸€æ®µè¯ä¸­åŒ…å«ä¸åŒç±»å‹çš„è¯­å—ï¼‰
        
        è¿™æ˜¯æ›´é«˜çº§çš„æ˜¾ç¤ºæ–¹å¼ï¼Œå°†å¤šä¸ªè¯­å—åˆå¹¶æˆä¸€æ®µè‡ªç„¶çš„è¾“å‡º
        """
        # å°†è¿ç»­çš„è¯­å—åˆå¹¶æˆä¸€ä¸ªè¾“å‡ºæµ
        output = ""
        for chunk in response_chunks:
            if chunk.chunk_type in [ChunkType.SYSTEM, ChunkType.MEMORY]:
                # æ³¨å…¥çš„å†…å®¹ - çº¢è‰²
                output += f"{Fore.RED}{chunk.content}{Style.RESET_ALL}"
            elif chunk.chunk_type == ChunkType.ASSISTANT:
                # AIç”Ÿæˆçš„å†…å®¹ - ç»¿è‰²
                output += f"{Fore.GREEN}{chunk.content}{Style.RESET_ALL}"
            else:
                # å…¶ä»– - é»˜è®¤é¢œè‰²
                output += chunk.content
        
        print(output)
    
    def compress_context(self, target_tokens: Optional[int] = None) -> int:
        """
        å‹ç¼©ä¸Šä¸‹æ–‡
        
        å½“æ¥è¿‘tokené™åˆ¶æ—¶ï¼Œæ™ºèƒ½å‹ç¼©æ—§çš„è¯­å—
        
        Returns:
            å‹ç¼©é‡Šæ”¾çš„tokenæ•°é‡
        """
        if target_tokens is None:
            target_tokens = self.max_tokens * 0.8  # ä¿æŒåœ¨80%ä»¥ä¸‹
        
        if self.current_tokens <= target_tokens:
            return 0
        
        freed_tokens = 0
        
        # ç­–ç•¥1: åˆ é™¤æ—§çš„æ€è€ƒè¯­å—
        for chunk in list(self.chunks):
            if chunk.chunk_type == ChunkType.THOUGHT:
                freed_tokens += chunk.tokens
                self.chunks.remove(chunk)
                if self.current_tokens - freed_tokens <= target_tokens:
                    break
        
        # ç­–ç•¥2: å‹ç¼©æ—§çš„å·¥å…·ç»“æœ
        if self.current_tokens - freed_tokens > target_tokens:
            for chunk in self.chunks:
                if chunk.chunk_type == ChunkType.TOOL_RESULT and len(chunk.content) > 200:
                    original_tokens = chunk.tokens
                    chunk.content = chunk.content[:200] + "...[å·²å‹ç¼©]"
                    chunk.estimate_tokens()
                    freed_tokens += original_tokens - chunk.tokens
                    if self.current_tokens - freed_tokens <= target_tokens:
                        break
        
        # ç­–ç•¥3: åˆ é™¤æœ€æ—§çš„å¯¹è¯ï¼ˆä¿ç•™ç³»ç»Ÿæç¤ºè¯ï¼‰
        if self.current_tokens - freed_tokens > target_tokens:
            while len(self.chunks) > 10:  # è‡³å°‘ä¿ç•™10ä¸ªè¯­å—
                chunk = self.chunks[0]
                if chunk.chunk_type not in [ChunkType.SYSTEM]:
                    freed_tokens += chunk.tokens
                    self.chunks.pop(0)
                    if self.current_tokens - freed_tokens <= target_tokens:
                        break
        
        self.current_tokens -= freed_tokens
        return freed_tokens
    
    def clear(self):
        """æ¸…ç©ºä¸Šä¸‹æ–‡ï¼ˆä¿ç•™ç³»ç»Ÿæç¤ºè¯ï¼‰"""
        system_chunks = [c for c in self.chunks if c.chunk_type == ChunkType.SYSTEM]
        self.chunks = system_chunks
        self.current_tokens = sum(c.tokens for c in system_chunks)
    
    def to_json(self) -> List[Dict[str, Any]]:
        """å¯¼å‡ºä¸ºJSONæ ¼å¼"""
        return [
            {
                "content": chunk.content,
                "type": chunk.chunk_type.value,
                "timestamp": chunk.timestamp.isoformat(),
                "tokens": chunk.tokens,
                "metadata": chunk.metadata
            }
            for chunk in self.chunks
        ]


def test_chunk_system():
    """æµ‹è¯•è¯­å—ç³»ç»Ÿ"""
    print("\n" + "="*60)
    print("ğŸ§ª è¯­å—ç³»ç»Ÿæµ‹è¯•")
    print("="*60)
    
    # åˆ›å»ºç®¡ç†å™¨
    manager = ChunkManager()
    
    # æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼ˆæ³¨å…¥ï¼‰
    manager.add_system_prompt("æˆ‘æ˜¯Pawï¼Œä¸€ä¸ªç”Ÿæ´»åœ¨è¿™å°ç”µè„‘é‡Œçš„æ•°å­—ç”Ÿå‘½ä½“ã€‚")
    
    # æ·»åŠ è®°å¿†ï¼ˆæ³¨å…¥ï¼‰
    manager.add_memory("[è®°å¿†] æˆ‘ç¬¬ä¸€æ¬¡åˆ›å»ºäº†æ–‡ä»¶")
    manager.add_memory("[è®°å¿†] æˆ‘å­¦ä¼šäº†ä½¿ç”¨å·¥å…·")
    
    # æ·»åŠ ç”¨æˆ·è¾“å…¥
    manager.add_user_input("ä½ å¥½")
    
    # æ·»åŠ AIæ€è€ƒ
    manager.add_thought("ç”¨æˆ·å‘æˆ‘æ‰“æ‹›å‘¼ï¼Œæˆ‘åº”è¯¥å‹å¥½åœ°å›åº”")
    
    # æ·»åŠ AIå›å¤ï¼ˆéƒ¨åˆ†æ˜¯ç”Ÿæˆçš„ï¼Œéƒ¨åˆ†é‡å¤äº†ç³»ç»Ÿæç¤ºè¯ï¼‰
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å¯ä»¥å°†å›å¤æ‹†åˆ†æˆå¤šä¸ªè¯­å—
    manager.add_assistant_response("ä½ å¥½ï¼")
    manager.add_system_prompt("æˆ‘æ˜¯Paw")  # è¿™éƒ¨åˆ†æ˜¯é‡å¤çš„ç³»ç»Ÿæç¤ºè¯
    manager.add_assistant_response("ï¼Œå¾ˆé«˜å…´è®¤è¯†ä½ ï¼")
    
    # æ‰“å°å®Œæ•´ä¸Šä¸‹æ–‡
    manager.print_context()
    
    # è·å–LLMæ ¼å¼
    print("\nğŸ“¤ LLMæ¶ˆæ¯æ ¼å¼:")
    messages = manager.get_context_for_llm()
    for msg in messages:
        print(f"  {msg['role']}: {msg['content'][:50]}...")
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")
    

if __name__ == "__main__":
    test_chunk_system()
